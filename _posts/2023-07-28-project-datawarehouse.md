---
title: 프로젝트 회고 | e-Commerce DW 구축기
author: guaqamole
date: 2023-07-28 13:00:00 +0900
categories: [Project, Marketboro]
tags: [DW, BigQuery, Airflow, Dimensional Modeling]
image:
  path: /common/torvalds.png
published: true
---

데이터 불모지인 유통 시장에서 큰 성공을 이뤄낸 마켓보로라는 회사에 Data Engineer로 입사 후, 처음으로 진행한 데이터 플랫폼 프로젝트다. 회사가 급성장함에 따라 **플랫폼 별로 흩어져 있는 데이터를 중앙화 해달라는 요구사항 이었다.**  데이터 사이즈와는 별개로 이건 해본 사람만 알 수 있는 난이도가 높은 작업이다.

**그중 나의 역할은 MySQL 데이터를 BigQuery로 적재 후, 전사적으로 사용할 DW를 모델링하고 구축 하는것이였다.**

Data Engineering에 대해 경험이 부족해서 그런지, 출근 할 때 마다 더 평소보다 긴장되었고 오랜만에 느껴보는 쫄깃함이 마음속에 공존했다. 누가 시키진 않았지만:

- 서비스 별로 도메인 지식과, DB 스키마, 데이터의 성격을 최대한 이해해야했고 (1달 걸림..)
- DW를 어떻게 모델링 해야할지 (책읽고 모델링하는데..1달)
- 분석가들의 데이터 니즈는 무엇인지 (항상..)
- 데이터를 어떻게 ETL 할지 (항상..)

출 퇴근시 매일 자료를 찾고 고민하며 살았던 3개월이였다.

**그중 가장 힘들었던건 부족한 레퍼런스였다**. Spring 처럼 자료가 많아서 한글로 검색하던 영어로 검색하던 블로그, 공식 문서가 쏟아져 나오는게 아니였다. 내가 유일하게 의지할 수 있었던건 Ralph Kimball의 명저 DataWarehouse Toolkit 이였다. 이 책은 한글 번역도 구하기가 힘들어 오랜만에 묵혀두었던 대학 시절 논문 노가다를 시전해야만 했으며 거의 교회 성경책 처럼 끼고 살았다.

역시 인생은 배움의 연속이라는걸 다시한번 느끼며 어려운 마음으로 프로젝트를 시작했다.

<br>

****

## Problem Statement

전사적으로 사용할 DW를 바닥 상태에서 구축하려면 다음 큰 문제들을 하나씩 풀어나가야 했다:

1. DW는 어디에 구축할것인가?
   1. Redshift vs BigQuery
2. Data Engineering - 어떻게 ETL 할것인가?
   1. Airflow DAG는 어떻게 작성할것인가?
   2. ELT vs ETL vs EtLT
   3. 증분 vs CDC
   4. 초기 데이터는 어떻게 Load 할것인가?
3. Dimensional Modelling - 어떻게 모델링 할것인가?
   1. Star vs Snowflake Schema
   2. Granularity
   3. Fact vs Dimension
   4. 어디까지 중복을 허용할 것인가?
4. Data Engineering - 어떻게 Transform 할것인가?
   1. BigQuery Insert vs Merge
   2. Partition
6. Data Engineering - 어떻게 DW를 운영할것인가?
   1. Data Integration 이슈 (중복, 누락) 발생시 어떻게 대처할까?
   1. 검증은 어떻게 할것인가?

등 정말 고민할게 너무나도 많았다. 사실 이걸 3개월만에 끝낼 수 있을까? 라는 걱정이 앞섰지만, 전사적으로 Data가 안정적으로 흘러가려면 "내가 진행한 프로젝트가 기간안에 무조건 마무리 되어야한다 라는 생각에 밤낮없이 작업했던거 같다.

## 초기 단계

스타트업은 시간이 생명이지만,  유한한 투자금 또한 생명이다. 투자가 많이 들어왔다고 해서 비용을 남발해서는 안되며, 엔지니어는 어떤 상황에서도 비용을 최적화 해야한다고 생각한다. 

또한 스타트업에선 데이터 조직이 개발 조직 만큼 당장 퍼포먼스를 낼 수 없기 때문에 초기부터 막대한 비용이 들어가면 리더분들이 난감해지기 때문에 "가장 저렴하고 효과적인 방안"을 우선적으로 생각했다.

### 1. DW는 어디에 구축할것인가?

현재 회사의 모든 서비스는 AWS 기반으로 구축 되어있다. Cloud 기반 Columnar Storage는 대표적으로 Amazon Redshift와 GCP BigQuery가 양대산맥이다. 

Redshift와 BigQuery를 둘다 사용해본 경험이 없기 때문에, 문서로만 이 둘의 스펙을 비교할 수 밖에 없었다. 하지만 나는 결국 BigQuery의 저렴한 비용과, Fully-managed 방식, 그리고 Google Platform과 높은 연계성에 매료되고 말았다..

물론 Redshift가 fully-managed가 아니란건 아니다. 하지만 redshift를 구성할 때 별도의 클러스터 관리가 필요하므로, 굳이 더 높은 비용 내고 성능 안정성을 위해 편의성을 포기할 순 없었다.

#### 결론

- 성능을 우선으로 고려하면 Redshift, 비용와 편이를 고려하면 BigQuery
- GA, Google spreadsheet 등 google platform과 연계 할 일이 잦다면 BigQuery

### 2. 어떻게 ETL 할것인가?

MySQL 데이터를 BigQuery로 적재하기로 결정 한 후 이제 "어떻게"의 문제에 직면했다. 머릿속에는 크게 두가지 방안이 당장 떠올랐다. 

#### Batch vs CDC

DB 부하와 비용이 적게드는 증분 방식과, DB Transaction log를 읽어 실시간으로 데이터를 Sync 하는 CDC (Change Data Capture) 방식을 고민했다. CDC 방식으로 결정 할 경우 별도의 스트리밍 처리(kafka 구축)가 필요하게 되어 프로젝트가 더 복잡해지고, 비용도 높아질것이다.  

무엇보다 제일 중요한건, 실시간 분석 니즈가 당장 필요하지 않은데 굳이 높은 비용을 지불하며 실시간으로 데이터를 가져올 필요가 없다는 것이였다. 물론 추후에 실시간 니즈가 생길 가능성도 있었지만, 성능보단 안정성있는 데이터 플랫폼을 만드는 것이 우선이였고, 실시간 분석은 신설된 데이터 조직에게 아직 걸음마도 떼지 않은 영유아에게 달려보라는것과 같았다. 

따라서, 이번 프로젝트에서는 한시간마다 증분 데이터를 적재하는 Batch 방식을 선택했다. 대신 구축에 사용되는 비용을 최소(최적)화하여 대표님과 리더분들을 기쁘게 해드리로 결심했다.

#### 초기 데이터 Loading

프로젝트를 진행하며 가장 궁금했던 순간이 "한 플랫폼의 전체 데이터는 얼마나 될까?"를 마주했던 순간이였다. 나는 그중 마켓보로의 오픈 마켓 식자재 플랫폼인 '식봄'의 데이터를 확인할 특권을 가졌었는데,  사실 실망을 좀 했었다. 

전체 데이터를 전부 뽑아보니 내가 생각하는 만큼의 1/ 10도 안되는것이였다..! 이래서 대기업이 아닌곳에서 대용량 데이터 다뤄봤다고 하는게 과장됬다는것이 나름 체감되었다. 이제 대용량 데이터가 아니라는것을 체감한 이상,  airflow과 python으로 충분히 ELT 할 수 있다고 생각이 들었다. 

![Desktop View](/230728/2.png)
_압축된 DB 데이터를 GCS로 Load하는 Process_

따라서, 초기 데이터는 네트워크 비용이 많이 발생할것으로 예상되어 데이터를 압축해서 GCS에 import 후 테이블로 Load 하고, 이후 증분 데이터는 pymysql 라이브러리로 별도의 압축 없이 memory에 적재 (시간별 전체 증분 데이터 메모리 사용량이 약 25mb..ㅜㅜ) 후 바로 bigquery table에 load 하는 방향으로 아키텍처를 그렸다.

#### Dag 작성 & ELT

우선 DAG를 작성하기 전 어떤 Operator를 사용 할 수 있는지 공식 문서들을 살펴보았다. 그 중

- BigQueryToMySqlOperator (google cloud)
- File_to_gcs (contrib)

가 후보에 들어왔다. 소스 코드를 살펴보니 gzip 으로 압축해주는 기능과, project_id와 dataset_id 만 제공하면 테이블을 생성해주는 기능이 이미 구현 되어있었다. boilerplate code를 작성할 필요 없다고 생각해 해당 operator들을 사용해보기로 결정했다.

### 3. 어떻게 모델링 할것인가?

DW를 구축한다는 뜻은 정규화된 데이터를 비정규화하여 비즈니스 프로세스가 올바르게 흘러가는지 확인하고, 분석에 용이한 형태로 바꾼다는 뜻이다. 

따라서, **정규화된 식봄 데이터를 그대로 가져오는것이 아니기 때문에 수용 가능한 정도의 데이터 불일치는 감수**하고, 대신 추이를 보는 용도라고 목표를 설정했었다.

### 4. 어떻게 Transform 할것인가?

식봄 ETL의 경우 RDS 쿼리 결과를 Bigquery로 바로 적재하지 않고 Temp Table을 생성한 후 앞서 반환된 DictCursor Object를 사용하여 쿼리 결과를 Insert 한다. 이후 Temp Table을 Target Table과 merge한다. 

Merge 기능은 Bigquery에서 공식적으로 지원하는 Upsert 기능을 의미한다. Target 테이블과 Merge후, Merge가 완료되면 해당 Temp Table을 삭제한다.

![Desktop View](/230728/3.png)
_BigQuery Merge 기능으로 Transform 하는 Process_

현재는 데이터를 client-side (airflow node) 메모리에 적재하여 BigQuery에 Load 하고있는데, 대용량 데이터에는 적합하지 않다. 클라이언트 버퍼에 저장되기 때문에 메모리를 크게 점유하므로 대용량의 경우 SSCursor를 사용하는것이 바람직하다. 

해당 커서를 사용하면 쿼리 결과가 서버사이드 (DB)에 저장되고 row를 하나씩 요청해서 클라이언트로 가져와 처리할 수 있는 아키텍처를 제공한다. 물론 DB의 부하가 증가하고 ETL 시간이 증가하겠지만,  상황에 따라 적합하게 사용 할 수도 있을거 같다. 빨리 SSCursor를 사용해 볼 날이 오면 좋겠다!

### 5. 어떻게 DW를 운영할것인가?

증분 데이터를 한시간에 한번만 가져올 경우, 데이터 정합성에 이슈가 생길 가능성이 존재하기에 한시간에 한번 수행하는 DAG와, 일별 전체 데이터를 가져오는 DAG를 추가하여 누락 없이 데이터를 가져올 수 있도록 설정했다. 

또한, DB 장애가 발생했을 경우 DW에 그대로 영향을 주지 않기 위해 만약 DB가 down 됬을경우, Airflow DAG는 Task를 skip 하도록 설정했다. 따라서 일정 기간동안 빈 공백이 생기므로 특정 기간 사이 데이터를 가져오고 싶을 땐 별도의 `interval DAG` 를 작성하여 데이터 정합성을 맞추었다.

- `etl.py` : 증분 데이터를 ETL 하는 Dag. **실행 주기는 한시간 간격 정각으로 설정했다.**

- `etl_daily.py` : 증분 데이터를 ETL 하는 Dag . **실행 주기는 1일 간격 새벽2시로 설정되어있다.**

- `etl_interval.py` : 특정 기간을 지정하여 ETL 하는 Dag . 실행주기는 Airflow에서 제공하는 실행주기인 `@once` 로 설정해놓았다.

<br>